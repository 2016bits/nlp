defaults:
  - encoder: hf_bert # defines encoder initialization parameters,if you change to policy,notice change the hf_bert model to bert_base_chinese
  - datasets: retriever_default # contains a list of all possible sources of queries for evaluation. Specific set is selected by qa_dataset parameter
  - ctx_sources: default_sources # contains a list of all possible passage sources. Specific passages sources selected by ctx_datatsets parameter

indexers:
  flat:
    _target_: scripts.indexer.faiss_indexers.DenseFlatIndexer

  hnsw:
    _target_: scripts.indexer.faiss_indexers.DenseHNSWFlatIndexer

  hnsw_sq:
    _target_: scripts.indexer.faiss_indexers.DenseHNSWSQIndexer

# the name of the queries dataset from the 'datasets' config group
# qa_dataset: policy_test
fc_dataset: fever_test
# nq_test, policy_test

# a list of names of the passages datasets from the 'ctx_sources' config group
# ctx_datatsets: [policy]
ctx_datasets: [wiki_pages]
# dpr_wiki or policy_test,look for default

#Glob paths to encoded passages (from generate_dense_embeddings tool)
# encoded_ctx_files: ["/home/zhouyc912/Policy_Retrive/DPR-main/outputs/2023-08-16/01-49-35/_*"]
encoded_ctx_files: ["/data/yangjun/fact/wikifact/data/Wikipedia/dpr/data/wikipages"]

# ["/home/zhouyc912/DPR-main/outputs/2023-08-10/15-32-21/_*"]
# 用绝对路径!
# default nq context: [/share/DPR-main/downloads/data/retriever_results/nq/single/wikipedia_passages_*.pkl]
# policy test: ["/share/DPR-main/downloads/data/retriever_results/policy/*.pkl"]

out_file: results.json
# "regex" or "string"
match: string
n_docs: 10
# n_docs: top_k returned passages
validation_workers: 16

# Batch size to generate query embeddings
batch_size: 128

# Whether to lower case the input text. Set True for uncased models, False for the cased ones.
do_lower_case: True

# The attribute name of encoder to use for queries. Options for the BiEncoder model: question_model, ctx_model
# question_model is used if this param is empty
# encoder_path:
encoder_path: ctx_model

# path to the FAISS index location - it is only needed if you want to serialize faiss index to files or read from them
# (instead of using encoded_ctx_files)
# it should point to either directory or a common index files prefix name
# if there is no index at the specific location, the index will be created from encoded_ctx_files
index_path:

kilt_out_file:

# A trained bi-encoder checkpoint file to initialize the model
# model_file: /home/zhouyc912/Policy_Retrive/DPR-main/outputs/2023-08-10/10-50-59/results/dpr_biencoder.25
model_file: /data/yangjun/fact/wikifact/data/Wikipedia/dpr/weights/dpr_biencoder.10

# nq default path: /share/DPR-main/downloads/checkpoint/retriever/single/nq/bert-base-encoder.cp
# nq my train: /share/DPR-main/downloads/checkpoint/retriever/single/nq/dpr_biencoder.10
# for policy: /home/zhouyc912/DPR-main/outputs/2023-08-10/01-23-32/results/dpr_biencoder.9

validate_as_tables: False

# RPC settings
rpc_retriever_cfg_file: 
# 上面这个还用不了;
rpc_index_id:
use_l2_conversion: False
use_rpc_meta: False
rpc_meta_compressed: False

indexer: flat

# tokens which won't be slit by tokenizer
special_tokens:

# TODO: move to a conf group
# local_rank for distributed training on gpus
local_rank: -1
global_loss_buf_sz: 150000
device:
distributed_world_size:
distributed_port:
no_cuda: False
n_gpu:
fp16: False

# For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']."
#        "See details at https://nvidia.github.io/apex/amp.html
fp16_opt_level: O1

